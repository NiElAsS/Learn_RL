{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T18:54:34.117490Z",
     "start_time": "2022-01-05T18:54:33.595420Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "open_tensorboard = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "Created: Jan 4 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T18:54:34.122872Z",
     "start_time": "2022-01-05T18:54:34.118251Z"
    }
   },
   "outputs": [],
   "source": [
    "class actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        '''\n",
    "        Init the actor network\n",
    "\n",
    "        args:\n",
    "        input_dim (int): input dimension\n",
    "        output_dim (int): output dimension\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(128, output_dim)\n",
    "        self.std_layer = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Forward pass\n",
    "\n",
    "        args:\n",
    "        state (torch.tensor): the state variable\n",
    "\n",
    "        '''\n",
    "        x = self.lin(state)\n",
    "\n",
    "        mu = torch.tanh(self.mu_layer(x)) * 2  # the action space is [-2, 2]\n",
    "        log_std = F.softplus(self.std_layer(x))\n",
    "        std = torch.exp(log_std)\n",
    "        # std = F.softplus(self.std_layer(x)).clamp(0.1, 1000)\n",
    "\n",
    "        distr = Normal(mu, std)\n",
    "        action = distr.sample()\n",
    "\n",
    "        return action, distr\n",
    "\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        '''\n",
    "        Init the  critic network\n",
    "\n",
    "        args:\n",
    "        input_dim (int): input dimension\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Forward pass\n",
    "\n",
    "        args:\n",
    "        state (torch.tensor): the state variable\n",
    "\n",
    "        '''\n",
    "        x = self.lin(state)\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T18:54:34.131233Z",
     "start_time": "2022-01-05T18:54:34.123700Z"
    }
   },
   "outputs": [],
   "source": [
    "class A2Cagent:\n",
    "    '''\n",
    "    actor critic agent\n",
    "    '''\n",
    "\n",
    "    def __init__(self, env, gamma, learning_rate, entropy_weight) -> None:\n",
    "        '''\n",
    "        init the agent\n",
    "        \n",
    "        paras:\n",
    "            env (gym.Env): An openAI Gym environment\n",
    "            gamma (float): discount factor\n",
    "            learning_rate (tuple): learning rate for (actor_lr, critic_lr)\n",
    "            entropy_weight (float): rate of weighting entropy in the loss funtion\n",
    "\n",
    "        '''\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # init the networks\n",
    "        self.actor = actor(\n",
    "            self.env.observation_space.shape[0], self.env.action_space.shape[0]).to(self.device)\n",
    "        self.critic = critic(\n",
    "            self.env.observation_space.shape[0]).to(self.device)\n",
    "\n",
    "        # init the optimizer\n",
    "        \n",
    "        # THE FUCKING LEARNING RATES MATTERRRRRRRRRRRRRRRRRRRR!!!!!!\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=learning_rate[0])\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=learning_rate[1])\n",
    "\n",
    "        # init the transition\n",
    "        self.transition = []\n",
    "        self.total_step = 0\n",
    "        \n",
    "        # init Tensorboard for visualization\n",
    "        self.writer = SummaryWriter('result/a2c')\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Select an action given a state\n",
    "        '''\n",
    "\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "\n",
    "        action, dist = self.actor(state)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        self.transition = [state, log_prob]\n",
    "\n",
    "        return action.clamp(-2.0, 2.0).cpu().detach().numpy()\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Take the action and record the information from env\n",
    "        '''\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        self.transition.extend([next_state, reward, done])\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "        update the network and return the loss value\n",
    "        '''\n",
    "\n",
    "        state, log_prob, next_state, reward, done = self.transition\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "\n",
    "        # calculate loss for updating critic network\n",
    "        mask = 1 - done\n",
    "        target_function = reward + self.gamma * self.critic(next_state) * mask\n",
    "        \n",
    "        pred_function = self.critic(state)\n",
    "        target_function = target_function.to(self.device).detach()\n",
    "        \n",
    "        critic_loss = F.smooth_l1_loss(pred_function, target_function)\n",
    "        \n",
    "        # update critic network\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # calculate advantage function and loss for actor\n",
    "        advantage = (target_function - pred_function).detach()\n",
    "        actor_loss = -advantage * log_prob + self.entropy_weight * (-log_prob)\n",
    "\n",
    "        # update actor network\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    def train(self, num_steps):\n",
    "        '''\n",
    "        Training\n",
    "        '''\n",
    "        self.actor_losses, self.critic_losses, self.scores = [], [], []\n",
    "        state = self.env.reset()\n",
    "        num_done = 0\n",
    "        score = 0\n",
    "\n",
    "        with trange(num_steps) as pbar:\n",
    "            for idx in pbar:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                actor_loss, critic_loss = self.update()\n",
    "\n",
    "                # record the losses\n",
    "                self.writer.add_scalar(\"Actor_Losses\", actor_loss, idx)\n",
    "                self.writer.add_scalar(\"Critic_Losses\", critic_loss, idx)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                    # self.scores.append(score)\n",
    "                    self.writer.add_scalar(\"Scores\", score, num_done)\n",
    "                    num_done += 1\n",
    "                    score = 0\n",
    "                else:\n",
    "                    state = next_state\n",
    "        \n",
    "        # add all data to tensorboard\n",
    "        self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T18:54:34.133436Z",
     "start_time": "2022-01-05T18:54:34.131844Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 100\n",
    "NUM_STEPS = 100000\n",
    "GAMMA = 0.9\n",
    "ENTROPY_WEIGHT = 1e-2\n",
    "ACTOR_LEARNING_RATE = 1e-4\n",
    "CRITIC_LEARNING_RATE = 1e-3\n",
    "LEARNING_RATE = (ACTOR_LEARNING_RATE, CRITIC_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T18:54:35.469277Z",
     "start_time": "2022-01-05T18:54:34.133963Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = A2Cagent(env, GAMMA, LEARNING_RATE, ENTROPY_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T18:59:41.061191Z",
     "start_time": "2022-01-05T18:54:35.470239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 100000/100000 [05:05<00:00, 327.24it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train(NUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-05T19:03:54.706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard :\n",
    "    !tensorboard --logdir result/a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "998dca2299dd3a3ee31d1fbaf2c3f5775658bfb893764552c93f4bb7243dea52"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

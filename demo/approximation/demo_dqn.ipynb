{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecf66ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:21.382248Z",
     "start_time": "2022-01-30T14:34:20.841229Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "open_tensorboard = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac46505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:21.385598Z",
     "start_time": "2022-01-30T14:34:21.382934Z"
    }
   },
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.lin(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6a81fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:21.390732Z",
     "start_time": "2022-01-30T14:34:21.386588Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, states_size: int, buffer_size: int, batch_size: int = 32):\n",
    "\n",
    "        self.states_buffer = np.zeros(\n",
    "            (buffer_size, states_size), dtype=np.float32)\n",
    "        self.next_states_buffer = np.zeros(\n",
    "            (buffer_size, states_size), dtype=np.float32)\n",
    "        self.actions_buffer = np.zeros((buffer_size, ), dtype=np.float32)\n",
    "        self.rewards_buffer = np.zeros((buffer_size, ), dtype=np.float32)\n",
    "        self.done_buffer = np.zeros((buffer_size, ), dtype=np.float32)\n",
    "\n",
    "        self.max_buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.current_size = 0\n",
    "        self.ptr = 0\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        next_state: np.ndarray,\n",
    "        reward: float,\n",
    "        done: bool\n",
    "    ):\n",
    "        self.states_buffer[self.ptr] = state\n",
    "        self.next_states_buffer[self.ptr] = next_state\n",
    "        self.actions_buffer[self.ptr] = action\n",
    "        self.rewards_buffer[self.ptr] = reward\n",
    "        self.done_buffer[self.ptr] = done\n",
    "\n",
    "        self.current_size = min(self.current_size + 1, self.max_buffer_size)\n",
    "        self.ptr = (self.ptr + 1) % self.max_buffer_size\n",
    "\n",
    "    def sample_batch(self):\n",
    "        i = np.random.choice(\n",
    "            self.current_size, size=self.batch_size, replace=False)\n",
    "        return dict(states=self.states_buffer[i],\n",
    "                next_states=self.next_states_buffer[i],\n",
    "                actions=self.actions_buffer[i],\n",
    "                rewards=self.rewards_buffer[i],\n",
    "                done=self.done_buffer[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3cd3478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:21.399210Z",
     "start_time": "2022-01-30T14:34:21.391356Z"
    }
   },
   "outputs": [],
   "source": [
    "class dqnAgent:\n",
    "    def __init__(self,\n",
    "                 env: gym.Env,\n",
    "                 buffer_size: int,\n",
    "                 batch_size: int,\n",
    "                 target_update_period: int,\n",
    "                 epsilon_decay: float,\n",
    "                 max_epsilon: float = 1.0,\n",
    "                 min_epsilon: float = 0.01,\n",
    "                 gamma: float = 0.99\n",
    "                 ):\n",
    "\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        self.env = env\n",
    "        self.buffer = ReplayBuffer(self.state_dim, buffer_size, batch_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.eps = max_epsilon\n",
    "        self.eps_decay = epsilon_decay\n",
    "        self.max_eps = max_epsilon\n",
    "        self.min_eps = min_epsilon\n",
    "        self.target_update_period = target_update_period\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        self.dqn_net = network(self.state_dim, self.action_dim).to(self.device)\n",
    "        self.dqn_target_net = network(\n",
    "            self.state_dim, self.action_dim).to(self.device)\n",
    "        self.dqn_target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "        self.dqn_target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn_net.parameters())\n",
    "        self.transition = list()\n",
    "        \n",
    "        self.writer = SummaryWriter('result/dqn')\n",
    "\n",
    "    def select_action(self, state: np.ndarray):\n",
    "\n",
    "        if self.eps > np.random.random():\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            selected_action = self.dqn_net(\n",
    "                torch.FloatTensor(state).to(self.device)).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "\n",
    "        self.transition = [state, selected_action]\n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        self.transition += [next_state, reward, done]\n",
    "        self.buffer.save(*self.transition)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        samples = self.buffer.sample_batch()\n",
    "        loss = self.compute_loss(samples)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def compute_loss(self, samples):\n",
    "\n",
    "        states = torch.FloatTensor(samples[\"states\"]).to(self.device)\n",
    "        next_states = torch.FloatTensor(samples[\"next_states\"]).to(self.device)\n",
    "        actions = torch.LongTensor(\n",
    "            samples[\"actions\"].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(samples[\"rewards\"]).reshape(-1, 1).to(self.device)\n",
    "        done=torch.FloatTensor(samples[\"done\"]).reshape(-1, 1).to(self.device)\n",
    "\n",
    "        # curr_q_values=self.dqn_net(states).gather(1, actions)\n",
    "        curr_q_values = torch.gather(self.dqn_net(states), 1, actions)\n",
    "        # use max()[0] bcs it returns required values and corresponding indices\n",
    "        next_q_values=self.dqn_target_net(next_states).max(\n",
    "            dim = 1, keepdim = True)[0].detach()\n",
    "\n",
    "        mask=1 - done\n",
    "        target=(rewards + self.gamma * next_q_values * mask).to(self.device)\n",
    "\n",
    "        loss=F.smooth_l1_loss(curr_q_values, target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self, num_episode):\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        update_count = 0\n",
    "        score = 0\n",
    "        \n",
    "        with trange(num_episode) as pbar:\n",
    "            for i in pbar:\n",
    "\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done = self.step(action)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                \n",
    "                \n",
    "\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                    self.writer.add_scalar(\"Scores\", score, i)\n",
    "                    score = 0\n",
    "\n",
    "                if len(self.buffer) >= self.buffer_size:\n",
    "\n",
    "                    loss = self.update()\n",
    "                    self.writer.add_scalar(\"Losses\", loss, i)\n",
    "                    update_count += 1\n",
    "\n",
    "                    curr_eps = self.eps - (self.max_eps - self.min_eps) * self.eps_decay\n",
    "                    self.eps = max(self.min_eps, curr_eps)\n",
    "\n",
    "                    if update_count % self.target_update_period == 0:\n",
    "                        self.dqn_target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "                \n",
    "        self.writer.flush()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464cbbfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:21.402859Z",
     "start_time": "2022-01-30T14:34:21.399909Z"
    }
   },
   "outputs": [],
   "source": [
    "env = \"CartPole-v0\"\n",
    "env = gym.make(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a4cb8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:21.405624Z",
     "start_time": "2022-01-30T14:34:21.403689Z"
    }
   },
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 128\n",
    "TARGET_UPDATE_PERIOD = 100\n",
    "EPSILON_DECAY = 1 / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bf505a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:22.733964Z",
     "start_time": "2022-01-30T14:34:21.406466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "agent = dqnAgent(env, BUFFER_SIZE, BATCH_SIZE, TARGET_UPDATE_PERIOD, EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d0bdbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T14:34:38.535430Z",
     "start_time": "2022-01-30T14:34:22.734817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:15<00:00, 633.04it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train(EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dbe90",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-30T14:34:20.837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard :\n",
    "    !tensorboard --logdir result/dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8306d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
